---
title: "Understanding the Factors That Drive House Prices"
author: 
  - Hitesh Yadav (hitesh2) 
  - Byunggeun Park (bpark14)
  - Avinika Pandugayala (avinika2)
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document: 
    theme: lumen
    toc: yes
    toc_float:
      collapsed: true
    highlight: tango
    code_folding: hide
    toc_collapsed: false
  pdf_document: default
urlcolor: cyan
bibliography: citation.bib
nocite: |
  @david
---

***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80)
library(knitr)
opts_chunk$set(cache = TRUE, autodep = TRUE)
```

## Introduction

### Overview

The housing market in today's day and age is influenced by a variety of factors like location, size of the house, amenities, school district and above all the economic conditions. The project is to predict the price of a house based on some of these factors. We aim to create a model using techniques in R and create a predictive model that can be used to estimate the price.

The data we are going to use is obtained from Kaggle and can be found at the link [here](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data) [@house-prices-advanced-regression-techniques]. We are not considering the economic conditions in this project as one of the factors for the price of the house and keeping it as out of scope for this analysis project.


## Methods

### Dataset

The data set we are using is sourced from a Kaggle competition in the link [here](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data) [@house-prices-advanced-regression-techniques].

From the competition Acknowledgements

*The Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset.*

#### Data files

The dataset is divided into 2 files that include training and test data. The dataset also includes a file that has description of the fields of the dataset.

```{r, message = FALSE, warning = FALSE}
library(readr)
require(plyr)
require(stringr)
housing_train = read.csv("data/train.csv")
housing_test = read.csv("data/test.csv")
data_desc = read.csv("data/field_descriptions.csv")
test_price = read.csv("data/sample_submission.csv")

# Combine the training and test files and price column for test data
housing_data = rbind(housing_train, merge(housing_test, test_price, by = "Id"))
```

- `train.csv` - Training dataset that contains ``r nrow(housing_train)`` observations and ``r ncol(housing_train)`` variables.
- `test.csv` - Test dataset that contains ``r nrow(housing_test)`` observations and ``r ncol(housing_test)`` variables.
- `data_description.csv` - Metadata for variables in the file. Originally prepared by Dean De Cock. [@house-prices-advanced-regression-techniques]
- `sample_submission.csv` - The file contains the sale price for the test data.

We will use the sale price values from `sample_submission.csv` to add the missing `SalePrice` variable in the test data and combine the data frames created from the train and test data. We are going to use a 80-20 split for the model building.

#### Variables/Data fields

```{r, echo = FALSE}
library(knitr)
library(kableExtra)
library(DT)

variables = data.frame(
  Variable = names(housing_data),
  Type = sapply(housing_data, typeof),
  row.names = NULL
)

df = merge(x = variables, y = data_desc, by = "Variable")

df %>%
  kable(booktabs = T, longtable = TRUE, linesep = "") %>%
  kable_styling(latex_options = c("HOLD_position", "repeat_header")
                , position = "center"
                , bootstrap_options = c("striped", "hover", "condensed")
                , fixed_thead = T) %>%
  column_spec(1, bold = T) %>%
  column_spec(3, width = "30em") %>%
  scroll_box(height = "400px")
```

### Data Exploration

For any data analysis project its good to know the the underlying data and the values. Specially what are types of the data and fields we can use to do the prediction.

We will use `SalePrice` as our response for the model and any or all fields in the dataset that we see fit for the model building.

#### Sample Data
Exploring the data we can look at some of the columns and values in the dataset.

```{r, message = FALSE, warning = FALSE}
dplyr::sample_n(housing_data, 10) %>%
  dplyr::select(SalePrice
                , YearBuilt
                , LotArea
                , Neighborhood
                , Utilities
                , BldgType
                , Fireplaces
                , GarageType) %>%
  kable(booktabs = T, linesep = "") %>%
  kable_styling(latex_options = c("HOLD_position", "repeat_header")
                , position = "center"
                , bootstrap_options = c("striped", "hover", "condensed")
                , fixed_thead = T)
```

#### Missing Values
We will use `DataExplorer` package to look into the data and initial profiling of the data. The idea is to understand the data better and make sure that we have detailed column analysis before we start building the model.

```{r, message = FALSE, warning = FALSE, fig.align='center'}
library(ggplot2)

gtheme = theme(plot.title = element_text(hjust = 0.5),
        axis.line = element_line(colour = "black"), panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), panel.border = element_blank(),
        panel.background = element_blank(), text = element_text(size=10))

library(DataExplorer)
t(introduce(housing_data)) %>%
  kable(booktabs = T, linesep = "") %>%
  kable_styling(latex_options = c("HOLD_position", "repeat_header")
                , position = "center"
                , bootstrap_options = c("striped", "hover", "condensed")
                , fixed_thead = T)
```

Looking at the initial table above we can see that there are substantial missing values in the observations specially there are `0` complete rows that means we cannot just remove the missing value lines, doing so will have 0 observations for training. We will circle back on fixing the missing values in some time.

```{r, message = FALSE, warning = FALSE, fig.align='center', fig.height=3.5, fig.width=7}
plot_intro(housing_data, ggtheme = gtheme)
```

The intro plot also provides similar insights that we have`0%` complete and `5.9%` of the observations have missing values. We can further check the features that have missing values.

```{r, message=FALSE, warning=FALSE, fig.align='center', fig.height=7, fig.width=7}
plot_missing(housing_data, missing_only = TRUE, ggtheme = gtheme)
```

Above plot provides details on the columns have missing values. We can further dive into the missing values variables by creating a bar plot and checking if the missing values need to be updated.

```{r, message = FALSE, warning = FALSE, fig.align='center'}
missing_profile = profile_missing(housing_data)
missing_only = missing_profile[missing_profile$num_missing > 0,]
housing_missing_cols = housing_data[, names(housing_data) %in% missing_only$feature]

plot_bar(housing_missing_cols
         , ggtheme = gtheme)

select_na_recs = rbind(dplyr::sample_n(housing_missing_cols[is.na(housing_missing_cols$LotFrontage),], 5)
                       , dplyr::sample_n(housing_missing_cols[is.na(housing_missing_cols$MasVnrArea),], 5)
                       , dplyr::sample_n(housing_missing_cols[is.na(housing_missing_cols$GarageYrBlt),], 5))

select_na_recs %>%
  dplyr::select(LotFrontage
                , GarageYrBlt
                , MasVnrArea) %>%
  mutate(LotFrontage = cell_spec(LotFrontage
                                 , color = "white"
                                 , background = ifelse(is.na(LotFrontage), "red", "green"))) %>%
  mutate(GarageYrBlt = cell_spec(GarageYrBlt
                                 , color = "white"
                                 , background = ifelse(is.na(GarageYrBlt), "red", "green"))) %>%
  mutate(MasVnrArea = cell_spec(MasVnrArea
                                , color = "white"
                                , background = ifelse(is.na(MasVnrArea), "red", "green"))) %>%
  kable(booktabs = T, linesep = "", escape=FALSE) %>%
  kable_styling(latex_options = c("HOLD_position", "repeat_header")
                , position = "center"
                , bootstrap_options = c("striped", "hover", "condensed")
                , fixed_thead = T)
```

Looking at the bar plot for the Categorical variables we can see that there are a lot of records with `NA` as value. Even though the data descriptions says that the NA is not applicable this value might not work well with `R`.

Similarly we can see that the numerical fields `LotFrontage`, `GarageYrBlt` and `MasVnrArea` we have `NA` as value. In this case even though `NA` means not applicable for the data we would need to make sure that we handle these values correctly in programming.

### Data Cleaning

To handle this scenario we will update the values for character variables to `Other` and for numerical variables to `0` so that we can handle them better when building the model.

```{r, message = FALSE, warning = FALSE, fig.align='center', fig.height=3.5, fig.width=7}
# Fix the missing values
housing_data[c("LotFrontage"
                , "GarageYrBlt"
                , "MasVnrArea")][is.na(housing_data[c("LotFrontage"
                                                       , "GarageYrBlt"
                                                       , "MasVnrArea")])] = 0

housing_data[names(housing_missing_cols)][is.na(housing_data[names(housing_missing_cols)])] = "Other"

plot_intro(housing_data, ggtheme = gtheme)

dplyr::sample_n(housing_data, 10) %>%
  dplyr::select(LotFrontage
                , GarageYrBlt
                , MasVnrArea) %>%
  kable(booktabs = T, linesep = "") %>%
  kable_styling(latex_options = c("HOLD_position", "repeat_header")
                , position = "center"
                , bootstrap_options = c("striped", "hover", "condensed")
                , fixed_thead = T)
```
With the initial data cleaning done we can see from above plot that the data seems to be clean and usable now as we have removed all the `NA` and all rows are complete with values that we can use in `R` for modeling.

### Response Analysis

We are going to use `SalePrice` as response for the model. As this is a continuous variable we would want to investigate the distribution of the price so that we we can use a linear modeling technique to predict the price. A good initial tool is to check the distribution for a continuous variable is to plot a histogram.

```{r, message = FALSE, warning = FALSE, fig.align='center', fig.height=3.5, fig.width=7}
ggplot(housing_data, aes(x = SalePrice)) +
  geom_histogram(bins=30, fill = "coral") +
  labs(title = "Sale Price Distribution", 
       x = "Sale Price", y = "Frequency") +
  theme(plot.title = element_text(hjust = 0.5),
        axis.line = element_line(colour = "black"), panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), panel.border = element_blank(),
        panel.background = element_blank(), text = element_text(size=8))
```
Looking at the histogram we can see that the price of the houses is Skewed to the left. A lot of the time doing a log transformation on the response helow with model building. That is we can try doing a couple of transformations of the `SalePrice` and check if the values follow a normal distribution and hence helping us build a linear model.

```{r, message = FALSE, warning = FALSE, fig.align='center', fig.height=3.5, fig.width=7, fig.cap="Response transformation comparision."}
library(gridExtra)

test_response_df = data.frame(logPrice = log(housing_data$SalePrice)
                              , sqrtPrice = sqrt(housing_data$SalePrice)
                              , cubertPrice = housing_data$SalePrice^(1/3))

plot1 = ggplot(test_response_df, aes(x = logPrice)) +
  geom_histogram(bins=30, fill = "coral") +
  labs(title = "Sale Price Distribution", 
       x = "logSalePrice", y = "Frequency") +
  theme(plot.title = element_text(hjust = 0.5),
        axis.line = element_line(colour = "black"), panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), panel.border = element_blank(),
        panel.background = element_blank(), text = element_text(size=8))

plot2 = ggplot(test_response_df, aes(x = sqrtPrice)) +
  geom_histogram(bins=30, fill = "coral") +
  labs(title = "Sale Price Distribution", 
       x = "sqrtSalePrice", y = "Frequency") +
  theme(plot.title = element_text(hjust = 0.5),
        axis.line = element_line(colour = "black"), panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), panel.border = element_blank(),
        panel.background = element_blank(), text = element_text(size=8))

plot3 = ggplot(test_response_df, aes(x = cubertPrice)) +
  geom_histogram(bins=30, fill = "coral") +
  labs(title = "Sale Price Distribution", 
       x = "cubertSalePrice", y = "Frequency") +
  theme(plot.title = element_text(hjust = 0.5),
        axis.line = element_line(colour = "black"), panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), panel.border = element_blank(),
        panel.background = element_blank(), text = element_text(size=8))

grid.arrange(plot1, plot2, plot3, ncol=3)
```
Out of the 3, log, square root and cube root transformations, we can see that log transforming the response variable gives us the best result of **normal distribution** of the response variable `SalePrice`. We can now confirm that `SalePrice` should be log transformed in the model so that we can normalize the distribution.

### Factor variables
A lot of the variables in the dataset are read as character. We will coerce them to factor so that its easier for model building with categorical data.
```{r, message=FALSE, warning=FALSE}
# Coerce string variables as factor
housing_data[sapply(housing_data, is.character)] = lapply(housing_data[sapply(housing_data, is.character)], as.factor)
```

### Model Building
Before we can build or model we need to be sure on what features we are going to use. There are 80 variables with mix of continuous and categorical types. Not all variables will be useful and we can use a process like correlation analysis for numeric variables to drop some of the variables that are highly correlated. We will also look at the categorical variables and drop some variables which do not add value to the model building.

#### Feature Selection

##### Numeric variables
As there are many numeric and categorical variables we can are going to use correlation analysis to select features which are highly correlated to sale price and then compare then to other features which can be dropped.
```{r, message = FALSE, warning = FALSE, fig.align='center', fig.height=6, fig.width=7, fig.cap="Numeric data correlation with SalePrice"}

# Find Correlation between numeric variables
plot_correlation(housing_data, type = "continuous", ggtheme = gtheme)
```

Looking at the correlation plot above we can see that there are a number of numeric variables that are highly correlated to the the response `SalePrice`. The variables also so a great correlation between themselves and we can eliminate variables which are highly correlated before building the model.

```{r, message = FALSE, warning = FALSE}
library(dplyr)
# Select numerical columns
numerical_cols = subset(housing_data, select = -c(SalePrice)) %>% select_if(is.numeric)

cor_matrix = cor(numerical_cols)
# Find the pairs of variables with high correlation
cor_matrix[upper.tri(cor_matrix)] = 0
diag(cor_matrix) = 0

high_cor_pairs = which(cor_matrix > 0.3 & cor_matrix < 1 , arr.ind = TRUE)

selected_numeric = c()
for (i in 1:nrow(high_cor_pairs)) {
  row_index = high_cor_pairs[i, 1]
  col_index = high_cor_pairs[i, 2]
  var1 = colnames(cor_matrix)[row_index]
  var2 = colnames(cor_matrix)[col_index]
  correlation_value = cor_matrix[row_index, col_index]
  #print(sprintf("Variables: %s and %s Correlation: %.20f", var1, var2, correlation_value))
  selected_numeric = c(selected_numeric, var1)
}
```

Taking a threshold of `0.6` for the highly correlated variables of numeric type we can see that the columns we can select are: **`r unique(selected_numeric)`**. We can validate that the variables have a good linear relationship with `SalePrice` by plotting as scatter plot.

```{r, message = FALSE, warning = FALSE, fig.align='center', fig.height=5, fig.width=7, fig.cap="Numeric variables vs. SalePrice"}
plot_data = housing_data[, c("SalePrice", unique(selected_numeric))]
plot_scatterplot(plot_data
                 , by = "SalePrice"
                 , ggtheme = gtheme
                 , geom_point_args = list(colour = "coral")
                 , nrow = 3L
                 , ncol = 4L)
```

##### Categorical Variables
For the categorical variables we will check and analyze how many of the variables have better coverage of the data for different levels. Looking at the bar plot for categorical variables in housing data above (section Missing Values) we can see that a lot of categorical variables have issue. That is either most of the data are in the same category or we have a very small sample of the data for a given category.

We will keep the threshold of the categorical variables as 80 and 20. As there is no generalized level for the categorical variables we can start with any level and then experiment. We can then perform the significance test for each of the individual variables and select those which have a significant impact on the response `SalePrice` at a significance level of `0.05`.

```{r, message = FALSE, warning = FALSE}
categorical_cols = housing_data %>% select_if(is.factor)
categorical_cols_names = names(categorical_cols)
all_categories = housing_data[, names(categorical_cols)]

selected_categories = c()

for (name in names(categorical_cols)) {
  max_proportion = max(prop.table(table(all_categories[name])))
  
   if (max_proportion < 0.8 & max_proportion > 0.5) {
    selected_categories = c(selected_categories, name)
   }
}

# Doing anova for the selected columns
anova_results = lapply(categorical_cols, function(selected_categories) {
  anova_model = lm(housing_data$SalePrice ~ selected_categories, data = housing_data)
  anova_result = anova(anova_model)
  return(anova_result)
})

# Extract F-statistic and p-value for each categorical variable
anova_summary = lapply(anova_results, function(anova_result) {
  F_Value = anova_result$"F value"[1]
  P_Value = anova_result$"Pr(>F)"[1]
  return(c(F_Value, P_Value))
})

# Combine ANOVA results into a data frame
anova_results_df = data.frame(
  Variable = names(anova_results),
  F_Value = sapply(anova_summary, function(x) x[1]),
  P_Value = sapply(anova_summary, function(x) x[2])
)

remove_categories = rownames(anova_results_df[anova_results_df$P_Value > 0.05, ])

selected_categories = selected_categories[!(remove_categories %in% selected_categories)]
```


The selected categorical features for the model are: **`r selected_categories`**

#### Feature List

Based on our feature selection we will use below features for the initial model.

**`r ordered(c(unique(selected_numeric), selected_categories))`**


#### Split Training and Test Data

Now that data is clean and prepped we will split the data into 80-20 split. `80%` of the data will be used as training data and `20%` of the data will be used for testing.

```{r, message = FALSE, warning = FALSE}
set.seed(873465)
# Split data into training and test datasets
selected_data = subset(housing_data, select = c("Id", "SalePrice", unique(selected_numeric), selected_categories))
training_data = selected_data %>% dplyr::sample_frac(0.80)
test_data = dplyr::anti_join(selected_data, training_data, by = 'Id')

# Drop Id Column from training and test data
training_data = subset(training_data, select = -c(Id))
test_data = subset(test_data, select = -c(Id))
```

#### Additive model
We will start with a simple additive model with selected variables and perform a backward AIC to reduce the model and select which of the variables perform well.

```{r, message = FALSE, warning = FALSE}
lm_housing_add = lm(log(SalePrice) ~ ., data = training_data)
lm_housing_add_select = step(lm_housing_add, direction = "backward", trace = 0)
```

#### Two-way interaction model

Looking at the data and selected variables based on our analysis we think that there should be some good interactions between variables like `HouseStyle`, `LotShape`, `LotConfig`, `BedroomAbvGr`, `TotRmsAbvGrd`, `GrLivArea`, `GarageCars`, `MSZoning`, `X1stFlrSF`, `X2ndFlrSF`, `YearBuilt` and `YearRemodAdd` with certain variables. We will explore these interactions to build the initial interaction model and then reduce the model using backward AIC.

```{r, message = FALSE, warning = FALSE}
lm_housing_2_interac = lm(log(SalePrice) ~ X2ndFlrSF + X1stFlrSF + YearBuilt + YearRemodAdd 
                          + MasVnrArea + GrLivArea + FullBath + TotRmsAbvGrd + Fireplaces 
                          + HalfBath + BedroomAbvGr + OpenPorchSF + MSZoning + LotShape 
                          + LotConfig + HouseStyle + RoofStyle + MasVnrType + ExterQual 
                          + BsmtExposure + HeatingQC + BsmtFullBath + KitchenQual + GarageType 
                          + GarageCars + HouseStyle:LotShape + HouseStyle:LotConfig + HouseStyle:BedroomAbvGr
                          + HouseStyle:TotRmsAbvGrd + HouseStyle:GrLivArea + HouseStyle:GarageCars
                          + HouseStyle:MSZoning + HouseStyle:X1stFlrSF + HouseStyle:X2ndFlrSF + HouseStyle:YearRemodAdd
                          + LotShape:LotConfig + LotShape:BedroomAbvGr + LotShape:TotRmsAbvGrd + LotShape:GrLivArea
                          + LotShape:GarageCars + LotShape:MSZoning + LotShape:X1stFlrSF + LotShape:X2ndFlrSF + LotShape:YearBuilt
                          + LotShape:YearRemodAdd + LotConfig:BedroomAbvGr + LotConfig:TotRmsAbvGrd + LotConfig:GrLivArea
                          + LotConfig:GarageCars + LotConfig:MSZoning + LotConfig:X1stFlrSF + LotConfig:X2ndFlrSF
                          + LotConfig:YearBuilt + LotConfig:YearRemodAdd + BedroomAbvGr:TotRmsAbvGrd + BedroomAbvGr:GrLivArea 
                          + BedroomAbvGr:GarageCars + BedroomAbvGr:MSZoning + BedroomAbvGr:X1stFlrSF + BedroomAbvGr:X2ndFlrSF
                          + BedroomAbvGr:YearBuilt + BedroomAbvGr:YearRemodAdd + TotRmsAbvGrd:GrLivArea + TotRmsAbvGrd:GarageCars
                          + TotRmsAbvGrd:MSZoning + TotRmsAbvGrd:X1stFlrSF + TotRmsAbvGrd:X2ndFlrSF + TotRmsAbvGrd:YearBuilt 
                          + TotRmsAbvGrd:YearRemodAdd + GrLivArea:GarageCars + GrLivArea:MSZoning + GrLivArea:X1stFlrSF 
                          + GrLivArea:X2ndFlrSF + GrLivArea:YearBuilt + GrLivArea:YearRemodAdd + GarageCars:MSZoning
                          + GarageCars:X1stFlrSF + GarageCars:X2ndFlrSF + GarageCars:YearBuilt + GarageCars:YearRemodAdd
                          + MSZoning:X1stFlrSF + MSZoning:X2ndFlrSF + MSZoning:YearBuilt + MSZoning:YearRemodAdd 
                          + X1stFlrSF:X2ndFlrSF + X1stFlrSF:YearBuilt + X1stFlrSF:YearRemodAdd + X2ndFlrSF:YearBuilt
                          + X2ndFlrSF:YearRemodAdd + YearBuilt:YearRemodAdd, 
                          data = training_data)
lm_housing_interac_select = step(lm_housing_2_interac, direction = "backward", trace = 0)
```

#### Polynomial model

We can also build a polynomial model and look into how the variables perform. As there are many numeric variables that were selected during variable selection we should be able to try out the polynomial transformation for the variables and see if the model benefits from the transformation. We will only do a quadratic polinomial for the sake of simplicity of the project.
```{r, message = FALSE, warning = FALSE}
lm_housing_quad = lm(log(SalePrice) ~ I(X2ndFlrSF ^ 2) + I(X1stFlrSF ^ 2) + I(YearBuilt ^ 2) + I(YearRemodAdd ^ 2) 
                          + I(MasVnrArea ^ 2) + I(GrLivArea ^ 2) + FullBath + I(TotRmsAbvGrd ^ 2) + Fireplaces 
                          + HalfBath + BedroomAbvGr + I(OpenPorchSF ^ 2) + MSZoning + LotShape 
                          + LotConfig + HouseStyle + RoofStyle + MasVnrType + ExterQual 
                          + BsmtExposure + HeatingQC + BsmtFullBath + KitchenQual + GarageType 
                          + GarageCars, data = training_data)
lm_housing_quad_select = step(lm_housing_quad, direction = "backward", trace = 0)
```

### Model Selection
We will use adjusted $R^2$ to select the best model out of the ones created above.

```{r, message = FALSE, warning = FALSE}
adj_r2 = data.frame(
  "Additive_Model" = c("Adjusted_R2" = summary(lm_housing_add_select)$adj),
  "AIC_Additive_Final" = c("Adjusted_R2" = summary(lm_housing_add_select)$adj),
  "Interacton_Model" = c("Adjusted_R2" = summary(lm_housing_2_interac)$adj),
  "AIC_Interacton_Model_Final" = c("Adjusted_R2" = summary(lm_housing_interac_select)$adj),
  "Polynomial_Model" = c("Adjusted_R2" = summary(lm_housing_quad)$adj),
  "AIC_Polynomial_Model_Final" = c("Adjusted_R2" = summary(lm_housing_quad_select)$adj)
)

knitr::kable(t(adj_r2)) %>%
  kable_styling(latex_options = c("HOLD_position", "repeat_header")
                , position = "center"
                , bootstrap_options = c("striped", "hover", "condensed")
                , fixed_thead = T
                , full_width = F)
```
Our best model is reduced two way interaction model which is **`lm_housing_interac_select`** as this has the maximum adj $R^2$ value.

## Results

Now that we have the model selected we can use the model to do a prediction and use diagnostics to validate the assumptions of a linear model.

### Diagnostics

For diagnostics we can use the Residuals vs. Fitted plot and QQ plot to check if the model assumptions hold.

```{r, message=FALSE, warning=FALSE, fig.align='center', fig.height=10, fig.width=7, fig.cap="Model Diagnostics."}
plot_fitted_resid = function(model, pointcol = "coral", linecol = "dodgerblue") {
  plot(fitted(model), resid(model), 
       col = pointcol, pch = 20, cex = 1.5,
       xlab = "Fitted", ylab = "Residuals",
       main = "Residuals vs. Fitted")
  abline(h = 0, col = linecol, lwd = 2)
}

plot_qq = function(model, pointcol = "coral", linecol = "dodgerblue") {
  qqnorm(resid(model), col = pointcol, pch = 20, cex = 1.5)
  qqline(resid(model), col = linecol, lwd = 2)
}

par(mfrow=c(2,1))

plot_fitted_resid(lm_housing_interac_select)
plot_qq(lm_housing_interac_select)
```

As we can see from the plot that the variance assumption as well as normality assumptions are a suspect for the select interactive model. We can see that the residuals are not distributed evenly and the QQ plot seem to have fat tails. To further validate the results for Normality assumption we can do a Shapiro-Wilk test and find that the normality is indeed a suspect here. 

```{r, message = FALSE, warning=FALSE}
get_sw_decision = function(model, alpha) {
  decide = unname(shapiro.test(resid(model))$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_sw_decision(lm_housing_interac_select, alpha = 0.05)
```

As we can see that the **`r shapiro.test(resid(lm_housing_interac_select))$p.value`** is less than **$\alpha = 0.05$** hence we Reject the null hypothesis and conclude that there is evidence that the data is not normally distributed and say that Normality is a suspect. This is because of large standardized residual.

Lets analyze these influential points by using cooks distance and remove the data points from the training dataset to plot the diagnostics.

```{r, message=FALSE, warning=FALSE, fig.align='center', fig.height=10, fig.width=7, fig.cap="Model Diagnostics."}
mod_cd = cooks.distance(lm_housing_interac_select)

cleaned_interactive_mod = lm(log(SalePrice) ~ X2ndFlrSF + X1stFlrSF + YearBuilt + 
    YearRemodAdd + MasVnrArea + GrLivArea + FullBath + TotRmsAbvGrd + 
    Fireplaces + HalfBath + BedroomAbvGr + MSZoning + LotShape + 
    LotConfig + HouseStyle + MasVnrType + ExterQual + BsmtExposure + 
    HeatingQC + KitchenQual + GarageCars + MSZoning:HouseStyle + 
    LotShape:GarageCars + YearBuilt:LotShape + TotRmsAbvGrd:LotConfig + 
    GrLivArea:LotConfig + LotConfig:GarageCars + YearBuilt:LotConfig + 
    YearRemodAdd:LotConfig + BedroomAbvGr:GarageCars + BedroomAbvGr:MSZoning + 
    YearBuilt:BedroomAbvGr + TotRmsAbvGrd:GarageCars + TotRmsAbvGrd:MSZoning + 
    GrLivArea:GarageCars + GrLivArea:MSZoning + X1stFlrSF:GrLivArea + 
    YearBuilt:GrLivArea + YearRemodAdd:GrLivArea + MSZoning:GarageCars + 
    X1stFlrSF:GarageCars + X2ndFlrSF:GarageCars + YearBuilt:GarageCars + 
    X1stFlrSF:MSZoning + X2ndFlrSF:MSZoning + YearBuilt:MSZoning + 
    YearRemodAdd:MSZoning + X1stFlrSF:YearBuilt + X2ndFlrSF:YearBuilt + 
    YearBuilt:YearRemodAdd, data = training_data, subset = mod_cd < (4 / nrow(training_data)))

par(mfrow=c(2,1))

plot_fitted_resid(lm_housing_quad)
plot_qq(lm_housing_quad)
```

We can see with the cooks distance calculation that there are 176 influential points. To test we can remove the influential points and try fitting the same model that was selected from the larger interaction model. We can see that the issue persists that is we still have normality issue in the data.

We can see in the below table that removing the outliers does not really help the model/selected model as the adjusted $R^2$ remains fairly same with p-value increasing. However, removing the data means we could end up in a situation where we inadvertently remove a class of data that is not in training but is preset in test. At this moment we can still say that our interaction model is the best model fitted.
```{r, message = FALSE, warning = FALSE}
model_p_val = data.frame(
  "Interaction Model with Outliers" = c("P-Value" = shapiro.test(resid(lm_housing_interac_select))$p.value
                                        , "Adjusted R^2" = summary(lm_housing_interac_select)$adj),
  "Interaction Model with no Outliers" = c("P-Value" = shapiro.test(resid(cleaned_interactive_mod))$p.value
                                          ,"Adjusted R^2" = summary(cleaned_interactive_mod)$adj)
)

knitr::kable(t(model_p_val), digits = 32) %>%
  kable_styling(latex_options = c("HOLD_position", "repeat_header")
                , position = "center"
                , bootstrap_options = c("striped", "hover", "condensed")
                , fixed_thead = T
                , full_width = F)
```

### Model Performance
To check the model performance lets do RMSE, a lower RMSE indicates better model performance because it means the model's predictions are closer to the actual values. 

```{r, message = FALSE, warning=FALSE}
# Make predictions using the fitted model and test data
predicted_values = predict(lm_housing_interac_select, newdata = test_data)

test_data$log_SalePrice = log(test_data$SalePrice)
rmse = sqrt(mean((test_data$log_SalePrice - predicted_values)^2))

test_data_rmse = data.frame(
  "Test" = c("RMSE" = rmse)
)

knitr::kable(t(test_data_rmse)) %>%
  kable_styling(latex_options = c("HOLD_position", "repeat_header")
                , position = "center"
                , bootstrap_options = c("striped", "hover", "condensed")
                , fixed_thead = T
                , full_width = F)
```

For graphical summaries of the results lets compare our predicted and actual values using a scatter plot.

```{r, message = FALSE, warning=FALSE, fig.align='center', fig.height=3.5, fig.width=7, fig.cap="Test Data - Predicted vs. Actuals."}
library(ggplot2)

comparison_data = data.frame(Predicted = predicted_values, Actual = test_data$log_SalePrice)
ggplot(comparison_data, aes(x = Actual, y = Predicted)) +
  geom_point(color = "coral") +
  geom_smooth(method="lm", col="dodgerblue", se=FALSE) +
  labs(x = "Actual Response", y = "Predicted Value", title = "Predicted vs Actual") +
  theme(plot.title = element_text(hjust = 0.5),
        axis.line = element_line(colour = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(), 
        text = element_text(size=8))

```

In this scatter plot, the points closely follow the regression line, it indicates that the predicted values closely match the actual response values.

```{r, message = FALSE, warning=FALSE, fig.align='center', fig.height=3.5, fig.width=7, fig.cap="Test Data - Residual Plot"}
residuals = test_data$log_SalePrice - predicted_values

# Create a residual plot
ggplot(data.frame(Residuals = residuals), aes(x = seq_along(Residuals), y = Residuals)) +
  geom_point(color = "coral") +
  geom_hline(yintercept = 0, color = "dodgerblue") +
  labs(x = "Observation Index", y = "Residuals", title = "Residual Plot") +
  theme(plot.title = element_text(hjust = 0.5),
        axis.line = element_line(colour = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(), 
        text = element_text(size=8))
```

In the residual plot, there is any pattern or trend in the residuals around the horizontal line, this indicates that the model has systematic errors.

## Discussion
The housing market is influenced by multiple factors such as location, house size, amenities, school district, and economic conditions. In this project, our aim was to predict house prices based on some of these factors using R techniques and create a reliable predictive model.

During the data exploration phase, we addressed missing values by creating bar graphs and making necessary adjustments. Once the dataset was cleaned, we conducted response analysis to assess the normal distribution of the target variable, `SalePrice.` We observed that log-transforming the `SalePrice` variable resulted in the best approximation to a normal distribution. Therefore, we confirmed the necessity of log-transforming `SalePrice` to achieve better model performance.

Furthermore, a correlation plot revealed several numeric variables that showed a strong correlation with `SalePrice.` Additionally, some variables displayed high correlation among themselves. To improve the model's efficiency and avoid multicollinearity, we eliminated highly correlated variables before constructing the final model.

For variable selection, we set a correlation threshold of 0.6 for numeric variables and performed significance tests for each categorical variable. We selected variables with a significant impact on the response variable, `SalePrice`, at a significance level of 0.05.

Subsequently, we explored various model types, including additive, two-way interaction, and polynomial models, using AIC for variable reduction. After considering adjusted R^2, we identified the two-way interaction model with the selected variables as the best-performing model.

The reduced two-way interaction model exhibited the largest adjusted $R^2$ value, indicating that it provides the closest predictions to the actual values. The predicted versus actual graph further confirmed the model's reliability. However, we noticed that this model violated the normality assumption based on the QQ plot. This observation was confirmed by the Shapiro-Wilk test, which rejected the null hypothesis of normality.

Despite the normality violation, our best model demonstrated close predictions of house prices. The selected model although the best could perform better if we include more variables and do multiple iteration of validations on the data.

In conclusion, our final model predicts house prices by incorporating important variables and can achieve satisfactory performance.

## Appendix
Below are some of the code and processes that we did not use in the final report.

1. Check distribution of variables in the dataset. We should benefit from checking the distribution of the different predictor variables in the data. Although we did not explore more on this we could have used the steps to get a better Feature selection.

```{r, message = FALSE, warning=FALSE}
plot_histogram(housing_data, scale_x = "log10", ggtheme = gtheme)
```

2. We also attempted a forward AIC test to check if we can reach variables that gives best result. Given the variability in the predictors and a large number of predictors we would need a lot of compute and longer model searching to arrive at a model that is best for the data analysis. Even though additive model generated with this code is better it is only slightly better than the interaction model in the report.
```{r, message = FALSE, warning=FALSE}
data = subset(housing_data, select = -c(Id))
lm_test = lm(log(SalePrice) ~ 1, data = data)
lm_full = lm(log(SalePrice) ~ . - SalePrice, data = data)

lm_select_fwd_aic = MASS::stepAIC(lm_test, direction = "forward"
                                  , scope = list(lower = lm_test, upper = lm_full)
                                  , trace = 0)
```

## References
